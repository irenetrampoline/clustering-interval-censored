{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Split into train and test data\n",
    "2. Train model on train data normally\n",
    "3. Take test data and duplicate into test prime \n",
    "4. Drop first visit from test prime data\n",
    "5. Get predicted delta from test prime data. Compare to delta from test data. We know the difference (epsilon) because we dropped actual visits. What percent of time is test delta < test prime delta? \n",
    "6. Restrict it only to patients with lot of visits. Is this better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def clean_plot():\n",
    "    ax = plt.subplot(111)    \n",
    "    ax.spines[\"top\"].set_visible(False)    \n",
    "    ax.spines[\"bottom\"].set_visible(False)    \n",
    "    ax.spines[\"right\"].set_visible(False)    \n",
    "    ax.spines[\"left\"].set_visible(False)    \n",
    "    \n",
    "    ax.get_xaxis().tick_bottom()    \n",
    "    ax.get_yaxis().tick_left()   \n",
    "    plt.grid()\n",
    "\n",
    "import matplotlib.pylab as pylab\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "#           'figure.figsize': (10,6),\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "pylab.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "sys.path.append('../data')\n",
    "from load import chf\n",
    "from data_utils import parse_data\n",
    "from synthetic_data import load_piecewise_synthetic_data\n",
    "\n",
    "\n",
    "sys.path.append('../model')\n",
    "from models import Sublign\n",
    "from run_experiments import get_hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_test_prime(test_data_dict_raw, drop_first_T=1.):\n",
    "    # drop first year\n",
    "    test_data_dict = copy.deepcopy(test_data_dict_raw)\n",
    "    eps_lst        = list()\n",
    "    \n",
    "    X = test_data_dict['obs_t_collect']\n",
    "    Y = test_data_dict['Y_collect']\n",
    "    M = test_data_dict['mask_collect']\n",
    "    \n",
    "    N_patients = X.shape[0]\n",
    "    N_visits   = X.shape[1]\n",
    "    \n",
    "    for i in range(N_patients):\n",
    "        eps_i = X[i,1,0] - X[i,0,0]\n",
    "        \n",
    "        first_visit = X[i,1,0]\n",
    "        # move all visits down (essentially destroying the first visit)\n",
    "        for j in range(N_visits-gap):\n",
    "            \n",
    "            X[i,j,0] = X[i,j+gap,0] - first_visit\n",
    "            Y[i,j,:] = Y[i,j+gap,:]\n",
    "            M[i,j,:] = M[i,j+gap,:]\n",
    "        \n",
    "        for g in range(1,gap+1):\n",
    "            X[i,N_visits-g,0] = int(-1000)\n",
    "            Y[i,N_visits-g,:] = int(-1000)\n",
    "            M[i,N_visits-g,:] = 0.\n",
    "        \n",
    "        eps_lst.append(eps_i)\n",
    "    return test_data_dict, eps_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data       = chf()\n",
    "max_visits = 38\n",
    "shuffle    = True\n",
    "num_output_dims = data.shape[1] - 4\n",
    "\n",
    "data_loader, collect_dict, unique_pid = parse_data(data.values, max_visits=max_visits)\n",
    "train_data_loader, train_data_dict, test_data_loader, test_data_dict, test_pid, unique_pid = parse_data(data.values, \n",
    "                                                                                                        max_visits=max_visits, test_per=0.2, \n",
    "                                                                                                        shuffle=shuffle)\n",
    "\n",
    "# model = Sublign(10, 20, 50, dim_biomarkers=num_output_dims, sigmoid=True, reg_type='l1', auto_delta=True, \n",
    "#                 max_delta=5, learn_time=True, device=torch.device('cuda'))\n",
    "# #         model.fit(data_loader, data_loader, args.epochs, 0.01, verbose=args.verbose,fname='runs/chf.pt',eval_freq=25)\n",
    "\n",
    "# fname='../model/chf_good.pt'\n",
    "# model.load_state_dict(torch.load(fname,map_location=torch.device('cuda')))\n",
    "\n",
    "\n",
    "test_p_data_dict, eps_lst = make_test_prime(test_data_dict, gap=1)\n",
    "\n",
    "\n",
    "# test_deltas   = model.get_deltas(test_data_dict).detach().numpy()\n",
    "# test_p_deltas = model.get_deltas(test_p_data_dict).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_output_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_test_prime(test_data_dict_raw, drop_first_T=1.):\n",
    "drop_first_T = 0.5\n",
    "# drop first year\n",
    "\n",
    "test_data_dict_new = copy.deepcopy(test_data_dict)\n",
    "eps_lst        = list()\n",
    "\n",
    "X = test_data_dict_new['obs_t_collect']\n",
    "Y = test_data_dict_new['Y_collect']\n",
    "M = test_data_dict_new['mask_collect']\n",
    "\n",
    "N_patients = X.shape[0]\n",
    "N_visits   = X.shape[1]\n",
    "\n",
    "remove_idx = list()\n",
    "\n",
    "X[X == -1000] = np.nan\n",
    "\n",
    "for i in range(N_patients):\n",
    "    N_visits_under_thresh = (X[i] < 0.5).sum()\n",
    "    gap = N_visits_under_thresh\n",
    "    \n",
    "    first_valid_visit     = X[i,N_visits_under_thresh,0]\n",
    "    \n",
    "    eps_i = X[i,N_visits_under_thresh,0]\n",
    "    \n",
    "    for j in range(N_visits-N_visits_under_thresh):\n",
    "        X[i,j,0] = X[i,j+gap,0] - first_valid_visit\n",
    "        Y[i,j,:] = Y[i,j+gap,:]\n",
    "        M[i,j,:] = M[i,j+gap,:]\n",
    "\n",
    "    for g in range(1,N_visits_under_thresh+1):\n",
    "        X[i,N_visits-g,0] = np.nan\n",
    "        Y[i,N_visits-g,:] = np.nan\n",
    "        M[i,N_visits-g,:] = 0.\n",
    "\n",
    "    if np.isnan(X[i]).all():\n",
    "        remove_idx.append(i)\n",
    "    else:\n",
    "        eps_lst.append(eps_i)\n",
    "\n",
    "keep_idx = [i for i in range(N_patients) if i not in remove_idx]\n",
    "X = X[keep_idx]\n",
    "Y = Y[keep_idx]\n",
    "M = M[keep_idx]\n",
    "\n",
    "print('Removed %d entries' % len(remove_idx))\n",
    "X[np.isnan(X)] = -1000\n",
    "\n",
    "#     eps_lst.append(eps_i)\n",
    "# return test_data_dict_new, eps_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_valid_visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dict_new = copy.deepcopy(test_data_dict)\n",
    "\n",
    "X = test_data_dict_new['obs_t_collect']\n",
    "Y = test_data_dict_new['Y_collect']\n",
    "M = test_data_dict_new['mask_collect']\n",
    "X[X == -1000] = np.nan\n",
    "\n",
    "i = 1\n",
    "N_visits_under_thresh = (X[i] < 0.5).sum()\n",
    "\n",
    "\n",
    "\n",
    "# for j in range(N_visits-N_visits_under_thresh):\n",
    "#     X[i,j,0] = X[i,j+gap,0] - first_visit\n",
    "#     Y[i,j,:] = Y[i,j+gap,:]\n",
    "#     M[i,j,:] = M[i,j+gap,:]\n",
    "\n",
    "# for g in range(1,N_visits_under_thresh+1):\n",
    "#     X[i,N_visits-g,0] = np.nan\n",
    "#     Y[i,N_visits-g,:] = np.nan\n",
    "#     M[i,N_visits-g,:] = 0.\n",
    "\n",
    "# if np.isnan(X[i]).all():\n",
    "#     print('yes')\n",
    "#     remove_idx.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X[1] < 0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_visits_under_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_visits_under_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(remove_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[X == -1000] = np.nan\n",
    "for i in range(10):\n",
    "    print(X[i].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(X.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y[1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dict_new['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('chf_experiment_results.pk', 'rb')\n",
    "results = pickle.load(f)\n",
    "test_deltas    = results['test_deltas']\n",
    "test_p_deltas  = results['test_p_deltas']\n",
    "eps_lst        = results['eps_lst']\n",
    "test_data_dict = results['test_data_dict']\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dict['obs_t_collect'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get num of visits per patient\n",
    "num_visits_patient_lst = list()\n",
    "for i in test_data_dict['obs_t_collect']:\n",
    "    num_visits = (i!=-1000).sum()\n",
    "    num_visits_patient_lst.append(num_visits)\n",
    "\n",
    "num_visits_patient_lst = np.array(num_visits_patient_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_visit_idx = np.where(num_visits_patient_lst > 10)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_p_deltas[freq_visit_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_deltas[freq_visit_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.array(test_p_deltas - test_deltas) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_p_deltas[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_plot()\n",
    "plt.plot(eps_lst, test_p_deltas - test_deltas, '.')\n",
    "plt.xlabel('Actual eps')\n",
    "plt.ylabel('Estimated eps')\n",
    "#     plt.savefig('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "\n",
    "def make_test_prime(test_data_dict_raw, gap=1):\n",
    "    test_data_dict = copy.deepcopy(test_data_dict_raw)\n",
    "    eps_lst        = list()\n",
    "    \n",
    "    X = test_data_dict['obs_t_collect']\n",
    "    Y = test_data_dict['Y_collect']\n",
    "    M = test_data_dict['mask_collect']\n",
    "    \n",
    "    N_patients = X.shape[0]\n",
    "    N_visits   = X.shape[1]\n",
    "    \n",
    "    for i in range(N_patients):\n",
    "        eps_i = X[i,1,0] - X[i,0,0]\n",
    "        \n",
    "        first_visit = X[i,1,0]\n",
    "        # move all visits down (essentially destroying the first visit)\n",
    "        for j in range(N_visits-gap):\n",
    "            \n",
    "            X[i,j,0] = X[i,j+gap,0] - first_visit\n",
    "            Y[i,j,:] = Y[i,j+gap,:]\n",
    "            M[i,j,:] = M[i,j+gap,:]\n",
    "        \n",
    "        for g in range(1,gap+1):\n",
    "            X[i,N_visits-g,0] = int(-1000)\n",
    "            Y[i,N_visits-g,:] = int(-1000)\n",
    "            M[i,N_visits-g,:] = 0.\n",
    "        \n",
    "        eps_lst.append(eps_i)\n",
    "    return test_data_dict, eps_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_prime_dict, eps_lst = make_test_prime(test_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_prime_dict['Y_collect'][1,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dict['Y_collect'][1,:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot successful model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from run_experiments import get_hyperparameters\n",
    "from models import Sublign\n",
    "\n",
    "sys.path.append('../data')\n",
    "\n",
    "from data_utils import parse_data\n",
    "from load import load_data_format\n",
    "\n",
    "sys.path.append('../evaluation')\n",
    "from eval_utils import swap_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dict['Y_collect'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dict['t_collect'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_Y = np.zeros((600,101,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_idx_dict = {'%.1f' % j: i for i,j in enumerate(np.linspace(0,10,101))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dict['obs_t_collect'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_t = np.round(train_data_dict['t_collect'],1)\n",
    "N, M, _ = rounded_t.shape\n",
    "\n",
    "for i in range(N):\n",
    "    for j in range(M):\n",
    "        val = rounded_t[i,j,0]\n",
    "#         try:\n",
    "        idx = val_idx_dict['%.1f' % val]\n",
    "        for k in range(3):\n",
    "            new_Y[i,idx,k] = train_data_dict['Y_collect'][i,j,k]\n",
    "#         except:\n",
    "#             print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(new_Y == 0).sum() / (600*101*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the files for comparing against SPARTan baseline\n",
    "\n",
    "for i in range(3):\n",
    "    a = new_Y[:,:,i]\n",
    "    np.savetxt(\"data1_dim%d.csv\" % i, a, deliREDACTEDer=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = train_data_dict['s_collect'][:,0]\n",
    "guess_labels = np.ones(600)\n",
    "\n",
    "adjusted_rand_score(true_labels,guess_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "# a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_format_num = 1\n",
    "# C, d_s, d_h, d_rnn, reg_type, lr = get_hyperparameters(data_format_num)\n",
    "anneal, b_vae, C, d_s, d_h, d_rnn, reg_type, lr = get_hyperparameters(data_format_num)\n",
    "C\n",
    "data = load_data_format(data_format_num, 0, cache=True)\n",
    "\n",
    "train_data_loader, train_data_dict, _, _, test_data_loader, test_data_dict, valid_pid, test_pid, unique_pid = parse_data(data.values, max_visits=4, test_per=0.2, valid_per=0.2, shuffle=False)\n",
    "\n",
    "model  = Sublign(d_s, d_h, d_rnn, dim_biomarkers=3, sigmoid=True, reg_type='l1', auto_delta=False, max_delta=0, learn_time=False, beta=0.00)\n",
    "model.fit(train_data_loader, test_data_loader, 800, lr, fname='runs/data%d_chf_experiment.pt' % (data_format_num), eval_freq=25)\n",
    "\n",
    "z = model.get_mu(train_data_dict['obs_t_collect'], train_data_dict['Y_collect'])\n",
    "# fname='runs/data%d_chf_experiment.pt' % (data_format_num)\n",
    "# model.load_state_dict(torch.load(fname))\n",
    "nolign_results = model.score(train_data_dict, test_data_dict)\n",
    "print('ARI: %.3f' % nolign_results['ari'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(anneal, b_vae, C, d_s, d_h, d_rnn, reg_type, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_format_num = 1\n",
    "# C, d_s, d_h, d_rnn, reg_type, lr = get_hyperparameters(data_format_num)\n",
    "anneal, b_vae, C, d_s, d_h, d_rnn, reg_type, lr = get_hyperparameters(data_format_num)\n",
    "\n",
    "model  = Sublign(d_s, d_h, d_rnn, dim_biomarkers=3, sigmoid=True, reg_type='l1', auto_delta=True, max_delta=5, learn_time=True, beta=0.01)\n",
    "model.fit(train_data_loader, test_data_loader, 800, lr, fname='runs/data%d.pt' % (data_format_num), eval_freq=25)\n",
    "\n",
    "z = model.get_mu(train_data_dict['obs_t_collect'], train_data_dict['Y_collect'])\n",
    "# fname='runs/data%d_chf_experiment.pt' % (data_format_num)\n",
    "# model.load_state_dict(torch.load(fname))\n",
    "results = model.score(train_data_dict, test_data_dict)\n",
    "print('ARI: %.3f' % results['ari'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model  = Sublign(d_s, d_h, d_rnn, dim_biomarkers=3, sigmoid=True, reg_type='l1', auto_delta=True, max_delta=5, learn_time=True, b_vae=0.)\n",
    "# model.fit(train_data_loader, test_data_loader, 800, lr, fname='runs/data%d_chf_experiment.pt' % (data_format_num), eval_freq=25)\n",
    "\n",
    "# z = model.get_mu(train_data_dict['obs_t_collect'], train_data_dict['Y_collect'])\n",
    "# # fname='runs/data%d_chf_experiment.pt' % (data_format_num)\n",
    "# # model.load_state_dict(torch.load(fname))\n",
    "# results = model.score(train_data_dict, test_data_dict)\n",
    "# print('ARI: %.3f' % results['ari'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize latent space (change configs above)\n",
    "X = test_data_dict['obs_t_collect']\n",
    "Y = test_data_dict['Y_collect']\n",
    "M = test_data_dict['mask_collect']\n",
    "\n",
    "\n",
    "test_z, _ = model.get_mu(X,Y)\n",
    "test_z    = test_z.detach().numpy()\n",
    "\n",
    "test_subtypes = test_data_dict['s_collect']\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "z_tSNE = TSNE(n_components=2).fit_transform(test_z)\n",
    "\n",
    "test_s0_idx = np.where(test_subtypes==0)[0]\n",
    "test_s1_idx = np.where(test_subtypes==1)[0]\n",
    "\n",
    "clean_plot()\n",
    "plt.plot(z_tSNE[test_s0_idx,0],z_tSNE[test_s0_idx,1],'.')\n",
    "plt.plot(z_tSNE[test_s1_idx,0],z_tSNE[test_s1_idx,1],'.')\n",
    "\n",
    "# plt.title('\\nNELBO (down): %.3f, ARI (up): %.3f\\n Config: %s\\nColors = true subtypes' % \n",
    "#           (nelbo, ari, configs))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_f(x, beta0, beta1):\n",
    "    result = 1. / (1+np.exp(-(beta0 + beta1*x)))\n",
    "    return result\n",
    "\n",
    "true_betas = [[[-4, 1],\n",
    "        [-1,1.],\n",
    "        [-8,8]\n",
    "        ],\n",
    "        [\n",
    "        [-1,1.],\n",
    "        [-8,8],\n",
    "        [-25, 3.5]\n",
    "        ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xs = np.linspace(0,10,100)\n",
    "\n",
    "for dim_i in range(3):\n",
    "    xs = np.linspace(0,10,100)\n",
    "    \n",
    "    plt.figure()\n",
    "    clean_plot()\n",
    "    plt.grid(True)\n",
    "    ys = [sigmoid_f(xs_i, true_betas[0][dim_i][0], true_betas[0][dim_i][1]) for xs_i in xs]\n",
    "    plt.plot(xs,ys, ':', color='gray', linewidth=5, label='True function')\n",
    "\n",
    "    ys = [sigmoid_f(xs_i, true_betas[1][dim_i][0], true_betas[1][dim_i][1]) for xs_i in xs]\n",
    "    plt.plot(xs,ys, ':', color='gray', linewidth=5)\n",
    "\n",
    "    for subtype_j in range(2):\n",
    "        \n",
    "\n",
    "        xs = np.linspace(0,10,100)\n",
    "        ys = [sigmoid_f(xs_i, nolign_results['cent_lst'][subtype_j,dim_i,0], \n",
    "                                nolign_results['cent_lst'][subtype_j,dim_i,1]) for xs_i in xs]\n",
    "        if subtype_j == 0:\n",
    "            plt.plot(xs,ys,linewidth=4, label='SubNoLign subtype', linestyle='-.', color='tab:green')\n",
    "        else:\n",
    "            plt.plot(xs,ys,linewidth=4, linestyle='--', color='tab:green')\n",
    "\n",
    "        ys = [sigmoid_f(xs_i, results['cent_lst'][subtype_j,dim_i,0], \n",
    "                                results['cent_lst'][subtype_j,dim_i,1]) for xs_i in xs]\n",
    "        if subtype_j == 0:\n",
    "            plt.plot(xs,ys,linewidth=4, label='SubLign subtype', linestyle='-', color='tab:purple')\n",
    "        else:\n",
    "            plt.plot(xs,ys,linewidth=4, linestyle='-', color='tab:purple')\n",
    "\n",
    "\n",
    "            \n",
    "    plt.xlabel('Disease stage')\n",
    "    plt.ylabel('Biomarker')\n",
    "    plt.legend()\n",
    "    plt.savefig('subnolign_data1_subtypes_dim%d.pdf' % dim_i, bbox_inches='tight')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # number dimensions\n",
    "# fig, axs = plt.subplots(1,3, figsize=(8,4))\n",
    "# for dim_i in range(3):\n",
    "#     ax = axs[dim_i]\n",
    "#     # number subtypes\n",
    "#     for subtype_j in range(2):\n",
    "#         xs = np.linspace(0,10,100)\n",
    "#         ys = [sigmoid_f(xs_i, model1_results['cent_lst'][subtype_j,dim_i,0], \n",
    "#                         model1_results['cent_lst'][subtype_j,dim_i,1]) for xs_i in xs]\n",
    "                \n",
    "\n",
    "#         ax.plot(xs,ys)\n",
    "#         ys = [sigmoid_f(xs_i, true_betas[0][dim_i][0], true_betas[0][dim_i][1]) for xs_i in xs]\n",
    "#         ax.plot(xs,ys, color='gray')\n",
    "        \n",
    "#         ys = [sigmoid_f(xs_i, true_betas[1][dim_i][0], true_betas[1][dim_i][1]) for xs_i in xs]\n",
    "#         ax.plot(xs,ys, color='gray')\n",
    "        \n",
    "# fig.suptitle('True data generating function (gray), learned models (orange, blue)')\n",
    "# plt.savefig('learned_models.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot CHF Delta distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open('../clinical_runs/chf_v3_1000.pk', 'rb'))\n",
    "clean_plot()\n",
    "plt.hist(data['deltas'], bins=20)\n",
    "plt.xlabel('Inferred Alignment $\\delta_i$ Value')\n",
    "plt.ylabel('Number Heart Failure Patients')\n",
    "plt.savefig('Delta_dist_chf.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make piecewise data to measure model misspecification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, 2*np.pi+np.pi/4, 2*np.pi/8)\n",
    "y = np.sin(x)\n",
    "tck = interpolate.splrep(x, y, s=0)\n",
    "xnew = np.arange(0, 2*np.pi, np.pi/50)\n",
    "ynew = interpolate.splev(xnew, tck, der=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = np.array([9.3578453 , 4.9814664 , 7.86530539, 8.91318433, 2.00779188])[sort_idx]\n",
    "yvals = np.array([0.35722491, 0.12512101, 0.20054626, 0.38183604, 0.58836923])[sort_idx]\n",
    "tck = interpolate.splrep(xvals, yvals, s=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_subtypes,D,N_pts,_ = subtype_points.shape\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3,nrows=1)\n",
    "\n",
    "for d, ax in enumerate(axes.flat):\n",
    "#     ax.set_xlim(0,10)\n",
    "#     ax.set_ylim(0,1)\n",
    "    for k in range(N_subtypes):\n",
    "        xs = subtype_points[k,d,:,0]\n",
    "        ys = subtype_points[k,d,:,1]\n",
    "        sort_idx = np.argsort(xs)\n",
    "        ax.plot(xs[sort_idx],ys[sort_idx])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# for d in range(D):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "N_epochs    = 800\n",
    "N_trials    = 5\n",
    "use_sigmoid = True\n",
    "\n",
    "sublign_results = {\n",
    "    'ari':[],\n",
    "    'pear': [],\n",
    "    'swaps': []\n",
    "}\n",
    "subnolign_results = {'ari': []}\n",
    "\n",
    "for trial in range(N_trials):\n",
    "    data_format_num = 1\n",
    "    # C, d_s, d_h, d_rnn, reg_type, lr = get_hyperparameters(data_format_num)\n",
    "    anneal, b_vae, C, d_s, d_h, d_rnn, reg_type, lr = get_hyperparameters(data_format_num)\n",
    "    # C\n",
    "    # data = load_data_format(data_format_num, 0, cache=True)\n",
    "\n",
    "    use_sigmoid = False\n",
    "\n",
    "    data, subtype_points = load_piecewise_synthetic_data(subtypes=2, increasing=use_sigmoid, \n",
    "                            D=3, N=2000,M=4, noise=0.25, N_pts=5)\n",
    "\n",
    "    train_data_loader, train_data_dict, _, _, test_data_loader, test_data_dict, valid_pid, test_pid, unique_pid = parse_data(data.values, max_visits=4, test_per=0.2, valid_per=0.2, shuffle=False)\n",
    "\n",
    "    model  = Sublign(d_s, d_h, d_rnn, dim_biomarkers=3, sigmoid=use_sigmoid, reg_type='l1', \n",
    "                     auto_delta=False, max_delta=5, learn_time=True, beta=1.)\n",
    "    model.fit(train_data_loader, test_data_loader, N_epochs, lr, fname='runs/data%d_spline.pt' % (data_format_num), eval_freq=25)\n",
    "\n",
    "    # z = model.get_mu(train_data_dict['obs_t_collect'], train_data_dict['Y_collect'])\n",
    "    # fname='runs/data%d_chf_experiment.pt' % (data_format_num)\n",
    "    # model.load_state_dict(torch.load(fname))\n",
    "    results = model.score(train_data_dict, test_data_dict)\n",
    "    print('Sublign results: ARI: %.3f; Pear: %.3f; Swaps: %.3f' % (results['ari'],results['pear'],results['swaps']))\n",
    "    sublign_results['ari'].append(results['ari'])\n",
    "    sublign_results['pear'].append(results['pear'])\n",
    "    sublign_results['swaps'].append(results['swaps'])\n",
    "    \n",
    "    model  = Sublign(d_s, d_h, d_rnn, dim_biomarkers=3, sigmoid=use_sigmoid, reg_type='l1', \n",
    "                     auto_delta=False, max_delta=0, learn_time=False, beta=1.)\n",
    "    model.fit(train_data_loader, test_data_loader, N_epochs, lr, fname='runs/data%d_spline.pt' % (data_format_num), eval_freq=25)\n",
    "    nolign_results = model.score(train_data_dict, test_data_dict)\n",
    "    print('SubNoLign results: ARI: %.3f' % (nolign_results['ari']))\n",
    "    subnolign_results['ari'].append(nolign_results['ari'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_str = 'Increasing' if use_sigmoid else 'Any'\n",
    "print('SubLign-%s & %.2f $\\\\pm$ %.2f & %.2f $\\\\pm$ %.2f & %.2f $\\\\pm$ %.2f \\\\\\\\' % (\n",
    "    data_str,\n",
    "    np.mean(sublign_results['ari']), np.std(sublign_results['ari']),\n",
    "    np.mean(sublign_results['pear']), np.std(sublign_results['pear']),\n",
    "    np.mean(sublign_results['swaps']), np.std(sublign_results['swaps'])\n",
    "))\n",
    "\n",
    "print('SubNoLign-%s & %.2f $\\\\pm$ %.2f & -- &  --  \\\\\\\\' % (\n",
    "    data_str,\n",
    "    np.mean(sublign_results['ari']), np.std(sublign_results['ari']),\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.score(train_data_dict, test_data_dict)\n",
    "print('Sublign results: ARI: %.3f; Pear: %.3f; Swaps: %.3f' % (results['ari'],results['pear'],results['swaps']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
